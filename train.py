"""
This code is used for training GraphDTI model. 
The input file should be the train and validation list generated by train_validion_list_generation.py 
, the label list of the input training data and the selected feature list index
The imput training data should be the integration of Mol2vec features for the drugs, ProtVec, Bionoi-AE and Graph2vec features for the proteins

@author: lgn
"""
import torch
from torch.utils import data
import argparse
import time
import numpy as np
import os
import json
import random
import pickle
from mlp_model import mlp_model
from data_generator import Dataset
import copy
# Seed everything
torch.manual_seed(123)
torch.cuda.manual_seed(123)
np.random.seed(123)
random.seed(123)
torch.backends.cudnn.enabled=False
torch.backends.cudnn.deterministic=True

def getArgs():
    parser = argparse.ArgumentParser()

    parser.add_argument('-data_dir', required=True, default='training_data/', help='input training data path')
    parser.add_argument('-bs', required=False, default=32, help='batch size, normally 2^n')
    parser.add_argument('-lr', required=False, default=0.0001, help='initial learning rate')
    parser.add_argument('-epoch', required=False, default=30, help='number of epoch for taining')
    parser.add_argument('-output', required=False,  default='models/', help='location for the result to be saved')
    parser.add_argument('-protocol', required=True, default='cluster', help='cluster protocol or random protocol')

    return parser.parse_args()

def split_train_valid(inx, protocol):

    if protocol == 'cluster':
        input_path = 'input_list_cluster/'
    else:
        input_path = 'input_list_random/'

    print(input_path)

    test_name = str(inx) + '_valid_list.pkl'
    print(test_name)
    with open(input_path + test_name, 'rb') as f:
        valid_list = pickle.load(f)


    file_list = os.listdir(input_path)
    file_list.remove(test_name)
    print(file_list)
    train_list_total = []
    for name in file_list:
        with open(input_path + name, 'rb') as f_tmp:
            train_list_tmp = pickle.load(f_tmp)
        train_list_total += train_list_tmp

    train_list = train_list_total
    # train_list = train_list_total[0:20000]
    print(len(train_list), len(valid_list))
    # print(valid_list[0], train_list[0])

    with open('training_label.pickle', 'rb') as f_label:
        labels = pickle.load(f_label)

    return train_list, valid_list, labels
    

def train_generator_cross_valid(datapath, batch_size, lr, num_epochs, output, prot):

    input_size = 400
    hidden_size = 128
    output_size = 2
    # load selected features list
    with open('permu_feature_importance.json') as json_file:
        feature_importance = json.load(json_file)
    feature_list = []
    score_list = []
    for name, improtance in feature_importance.items():
        feature_list.append(name)
        score_list.append(improtance)

    feature_tmp = feature_list[0:400]
    feature_sample = list(map(int, feature_tmp))

    params = {'batch_size': batch_size,
              'shuffle': True,
              'num_workers': 4}

    stage_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
    logs = {}

    for stage in stage_list:
        path_train = path_valid = datapath
        new_train_list, new_valid_list, labels = split_train_valid(stage, prot)
        print(len(new_train_list), len(new_valid_list))
        print(path_train, path_valid)
        partition = {"train": new_train_list, "validation": new_valid_list}
        # Generators
        training_set = Dataset(partition['train'], labels, path_train, feature_sample)
        training_generator = data.DataLoader(training_set, **params)
        validation_set = Dataset(partition['validation'], labels, path_valid, feature_sample)
        validation_generator = data.DataLoader(validation_set, **params)
        print('Training data is ready')

        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        mlp = mlp_model(input_size, hidden_size, output_size)
        mlp = mlp.to(device)
        print(mlp.parameters)
        # optimizer = torch.optim.Adam(mlp.parameters(), lr=lr, weight_decay=0.0001)
        optimizer = torch.optim.SGD(mlp.parameters(), lr=lr, momentum=0.9, weight_decay=0.00001)
        # optimizer = torch.optim.Adam(mlp.parameters(), lr=lr)
        criterion = torch.nn.CrossEntropyLoss()

        key_words = ['train_loss_' + str(stage), 'train_accuracy_' + str(stage), 'val_loss_' + str(stage),
                     'val_accuracy_' + str(stage)]
        logs[key_words[0]] = []
        logs[key_words[1]] = []
        logs[key_words[2]] = []
        logs[key_words[3]] = []
        best_val_loss = 9999999

        if not os.path.exists(output):
            os.makedirs(output)
        best_saved = str(stage) + '_mlp_supertarget_' + prot + '.pt'
        best_path = output + best_saved
        for epoch in range(num_epochs):
            print(epoch)
            train_acc_sum = 0
            train_loss_sum = 0.0
            val_acc_sum = 0
            val_loss_sum = 0.0
            mlp.train()

            for train_inputs, train_labels in training_generator:
                train_inputs, train_labels = train_inputs.to(device), train_labels.to(device)
                train_outputs = mlp(train_inputs.float())
                train_loss = criterion(train_outputs, train_labels)
                optimizer.zero_grad()  # zero the gradient buffer
                train_loss.backward()
                optimizer.step()
                _, train_predicted = torch.max(train_outputs, 1)  # find the max of softmax and map the predicted list
                train_loss_sum += train_loss.detach() * train_inputs.size(0)
                train_acc_sum += (train_predicted == train_labels.data).sum() # different from CPU
            # print(train_loss_sum.item())
            # print(train_acc_sum.item())
            train_loss_epoch = train_loss_sum.item() / len(training_set)
            train_acc_epoch = train_acc_sum.item() / len(training_set)
            if (epoch + 1) % 1 == 0:
                print('Epoch [{}/{}],  Training Loss:{}, Training Accuracy:{}'.format
                      (epoch + 1, num_epochs, train_loss_epoch,
                       train_acc_epoch))
            logs[key_words[0]].append(train_loss_epoch)
            logs[key_words[1]].append(train_acc_epoch)

            mlp.eval()
            torch.no_grad()
            for val_inputs, val_labels in validation_generator:
                val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)
                val_outputs = mlp(val_inputs.float())
                val_loss = criterion(val_outputs, val_labels)
                _, val_predicted = torch.max(val_outputs, 1)  # find the max of softmax and map the predicted list
                val_loss_sum += val_loss.detach() * val_inputs.size(0)
                val_acc_sum += (val_predicted == val_labels.data).sum()
            val_loss_epoch = val_loss_sum.item() / len(validation_set)
            val_acc_epoch = val_acc_sum.item() / len(validation_set)
            if (epoch + 1) % 1 == 0:
                print('Epoch [{}/{}],  Validation Loss:{}, Validation Accuracy:{}'.format
                      (epoch + 1, num_epochs, val_loss_epoch,
                       val_acc_epoch))
            logs[key_words[2]].append(val_loss_epoch)
            logs[key_words[3]].append(val_acc_epoch)

            if val_loss_epoch < best_val_loss:
                if os.path.exists(best_saved):
                    os.remove(best_saved)
                best_val_loss = val_loss_epoch
                best_val_loss_dict = {'train_loss': train_loss_epoch, 'train_acc': train_acc_epoch,
                                      'val_loss': val_loss_epoch, 'val_acc': val_acc_epoch}
                print('best val loss is', best_val_loss)
                best_mlp = copy.deepcopy(mlp)
                # paras = list(best_mlp.parameters())
                # for num, para in enumerate(paras):
                #     print('number:', num)
                #     print(para)
                torch.save(best_mlp, best_path)

        print('results at minimum val loss:')
        print(best_val_loss_dict)

    log_path = 'logs/'
    if not os.path.exists(log_path):
        os.makedirs(log_path)
    with open(log_path + prot + '_logs_mlp.json', 'w') as fp:
        json.dump(logs, fp)



if __name__ == "__main__":

    start = time.time()
    parse = getArgs()
    train_generator_cross_valid(parse.data_dir, parse.bs, parse.lr, parse.epoch,
                                parse.output, parse.protocol)
    end = time.time()
    print('vector time elapsed :' + str(end - start))
